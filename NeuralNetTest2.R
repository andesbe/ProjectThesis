#Reading the .csv files generated by the Matlab model
input <- read.csv(file="input.csv")
output <- read.csv(file="output.csv")

# Neural Network Visualization
library(keras)
library(mlbench) 
library(dplyr)
library(magrittr)
library(neuralnet)


output <-data.frame(output)
input <- data.frame(input)
str(output)
str(input)

data <- cbind(input, output)
            

colnames(data) <- c("alpha", "wamin", "Temp", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "y1", "y2", "y3", "Pt", "I")

#Drops the I variable, as it is not very interesting
data <- select(data, -17)
str(data)

#Normalizing data
alpha = (data$alpha-min(data$alpha))/(max(data$alpha)-min(data$alpha))
wamin = (data$wamin-min(data$wamin))/(max(data$wamin)-min(data$wamin))
Temp = (data$Temp-min(data$Temp))/(max(data$Temp)- min(data$Temp))

x1 = (data$x1-min(data$x1))/(max(data$x1)- min(data$x1))
x2 = (data$x2-min(data$x2))/(max(data$x2)- min(data$x2))
x3 = (data$x3-min(data$x3))/(max(data$x3)- min(data$x3))
x4 = (data$x4-min(data$x4))/(max(data$x4)- min(data$x4))
x5 = (data$x5-min(data$x5))/(max(data$x5)- min(data$x5))
x6 = (data$x6-min(data$x6))/(max(data$x6)- min(data$x6))
x7 = (data$x7-min(data$x7))/(max(data$x7)- min(data$x7))
x8 = (data$x8-min(data$x8))/(max(data$x8)- min(data$x8))
x9 = (data$x9-min(data$x9))/(max(data$x9)- min(data$x9))
y1 = (data$y1-min(data$y1))/(max(data$y1)- min(data$y1))
y2 = (data$y2-min(data$y2))/(max(data$y2)- min(data$y2))
y3 = (data$y3-min(data$y3))/(max(data$y3)- min(data$y3))
Pt = (data$Pt-min(data$Pt))/(max(data$Pt)- min(data$Pt))

data2 <- cbind(alpha, wamin, Temp, x1, x2, x3, x4, x5, x6, x7, x8, x9, y1, y2, y3, Pt)
# 
#Creating a training and validating set
set.seed(222)
ind <- sample(2, nrow(data2), replace = TRUE, prob = c(0.7, 0.3))
training <- data2[ind==1,]
testing <- data2[ind==2,]



n <- neuralnet(x1 + x2 + x3+ x4+ x5+ x6+ x7+ x8+ x9+ y1+ y2+ y3+ Pt ~ wamin + alpha + Temp,
               data = data2,
               hidden = c(50),
               threshold = 0.3)

#Plot a nice figure of the network that can be used in the report, while it generates weights, the neuralnet() package is so terrible it should never be used
#The only advantages it has are it's hardware compatibility and ease of implementation
plot(n,
     col.hidden = 'darkgreen',
     col.hidden.synapse = 'darkgreen',
     show.weights = F,
     information = F,
     fill = 'lightblue')

# Prediction
output <- predict(n, training)
validation <- predict(n, testing)

results <- validation
results2 <- matrix(unlist(results), ncol = 13, byrow = TRUE)
str(results2)
str(testing)
variables <- data.frame(results2)
colnames(variables) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "y1", "y2", "y3", "Pt")
variables



#Descaling the response variables

var1 <- variables$x1*(max(data$x1)-min(data$x1))+min(data$x1)
var2 <- variables$x2*(max(data$x2)-min(data$x2))+min(data$x2)
var3 <- variables$x3*(max(data$x3)-min(data$x3))+min(data$x3)
var4 <- variables$x4*(max(data$x4)-min(data$x4))+min(data$x4)
var5 <- variables$x5*(max(data$x5)-min(data$x5))+min(data$x5)
var6 <- variables$x6*(max(data$x6)-min(data$x6))+min(data$x6)
var7 <- variables$x7*(max(data$x7)-min(data$x7))+min(data$x7)
var8 <- variables$x8*(max(data$x8)-min(data$x8))+min(data$x8)
var9 <- variables$x9*(max(data$x9)-min(data$x9))+min(data$x9)
var10 <- variables$y1*(max(data$y1)-min(data$y1))+min(data$y1)
var11 <- variables$y2*(max(data$y2)-min(data$y2))+min(data$y2)
var12 <- variables$y3*(max(data$y3)-min(data$y3))+min(data$y3)
var13 <- variables$Pt*(max(data$Pt)-min(data$Pt))+min(data$Pt)





#Finding the MSE of the different variables
#First I must descale the validation set, luckily it was scaled with the same 

cont1 <- testing[,1]*(max(data$x1)-min(data$x1))+min(data$x1)
cont2 <- testing[,2]*(max(data$x2)-min(data$x2))+min(data$x2)
cont3 <- testing[,3]*(max(data$x3)-min(data$x3))+min(data$x3)
cont4 <- testing[,4]*(max(data$x4)-min(data$x4))+min(data$x4)
cont5 <- testing[,5]*(max(data$x5)-min(data$x5))+min(data$x5)
cont6 <- testing[,6]*(max(data$x6)-min(data$x6))+min(data$x6)
cont7 <- testing[,7]*(max(data$x7)-min(data$x7))+min(data$x7)
cont8 <- testing[,8]*(max(data$x8)-min(data$x8))+min(data$x8)
cont9 <- testing[,9]*(max(data$x9)-min(data$x9))+min(data$x9)
cont10 <- testing[,10]*(max(data$y1)-min(data$y1))+min(data$y1)
cont11 <- testing[,11]*(max(data$y2)-min(data$y2))+min(data$y2)
cont12 <- testing[,12]*(max(data$y3)-min(data$y3))+min(data$y3)
cont13 <- testing[,13]*(max(data$Pt)-min(data$Pt))+min(data$Pt)

mse1 <- mean((as.vector(cont1) -as.vector(var1))^2)
mse2 <- mean((as.vector(cont2) -as.vector(var2))^2)
mse3 <- mean((as.vector(cont3) -as.vector(var3))^2)
mse4 <- mean((as.vector(cont4) -as.vector(var4))^2)
mse5 <- mean((as.vector(cont5) -as.vector(var5))^2)
mse6 <- mean((as.vector(cont6) -as.vector(var6))^2)
mse7 <- mean((as.vector(cont7) -as.vector(var7))^2)
mse8 <- mean((as.vector(cont8) -as.vector(var8))^2)
mse9 <- mean((as.vector(cont9) -as.vector(var9))^2)
mse10 <- mean((as.vector(cont10) -as.vector(var10))^2)
mse11 <- mean((as.vector(cont11) -as.vector(var11))^2)
mse12 <- mean((as.vector(cont12) -as.vector(var12))^2)
mse13 <- mean((as.vector(cont13) -as.vector(var13))^2)

MSE <- cbind(c(mse1, mse2, mse3, mse4, mse5, mse6, mse7, mse8, mse9, mse10, mse11, mse12, mse13))
MSE

     