#Reading the .csv files generated by the Matlab model
input <- read.csv(file="input.csv")
output <- read.csv(file="output.csv")


# Neural Network Visualization
library(keras)
library(mlbench) 
library(dplyr)
library(magrittr)
library(neuralnet)

#Transforming the output to log scale, as this is usually better. This info comes form experience, not peer reviewed data.
output <- log(output)

#Scaling the input variables from 0 -> 1. The network converges easier and more accurately if the variables are in the same domain. 
alpha <- (input[,1]-min(input[,1]))/(max(input[,1])-min(input[,1]))
wamin <- (input[,2]-min(input[,2]))/(max(input[,2])-min(input[,2]))
Temp <- (input[,3]-min(input[,3]))/(max(input[,3])-min(input[,3]))


#Putting everything back into a dataframe and naming the columns. 
data <- cbind(alpha, wamin, Temp, output)
colnames(data) <- c("alpha", "wamin", "Temp", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "y1", "y2", "y3", "Pt")



#Splitting data into a training, and a validation set. And transforming them into the appropriate data type. In this case a matrix. 
dt = sort(sample(nrow(data), nrow(data)*.8))
train<-data[dt,]
test<-data[-dt,]


set.seed(1234)
training <- train[1:3]
testing <- test[1:3]
trainingtarget <- train[4:16]
testingtarget <- test[4:16]

x.train <- training
x.test <- testing

x.train <- as.matrix(x.train)
x.test <- as.matrix(x.test)

y.train <- as.matrix(trainingtarget)
y.test <- as.matrix(testingtarget)



#Creating the model itself
model <-keras_model_sequential()%>%
  layer_dense(units = 64, activation = "relu", input_shape =c(3))%>%
  layer_dropout(0.2) %>%
  layer_dense(units = 13, activation = "linear")

model%>%
  compile(optimizer = "ADAM", loss = "mean_absolute_percentage_error")

history <- model%>%
  fit(x.train, y.train, epochs = 1000, batch_size = 50, validation_split = 0.2)


#Evaluating the testing set, and transforming the predicted set back from the log scale 
model %>% evaluate(x.test, y.test)
pred <- model %>% predict(x.test)

pred <- exp(pred)
y.test <- exp(y.test)


#Finding Mean Absolute Percentage Error
mse1 <- abs(mean((as.vector(pred[,1]) -as.vector(y.test[,1]))/as.vector(pred[,1])))
mse2 <- abs(mean((as.vector(pred[,2]) -as.vector(y.test[,2]))/as.vector(pred[,2])))
mse3 <- abs(mean((as.vector(pred[,3]) -as.vector(y.test[,3]))/as.vector(pred[,3])))
mse4 <- abs(mean((as.vector(pred[,4]) -as.vector(y.test[,4]))/as.vector(pred[,4])))
mse5 <- abs(mean((as.vector(pred[,5]) -as.vector(y.test[,5]))/as.vector(pred[,5])))
mse6 <- abs(mean((as.vector(pred[,6]) -as.vector(y.test[,6]))/as.vector(pred[,6])))
mse7 <- abs(mean((as.vector(pred[,7]) -as.vector(y.test[,7]))/as.vector(pred[,7])))
mse8 <- abs(mean((as.vector(pred[,8]) -as.vector(y.test[,8]))/as.vector(pred[,8])))
mse9 <- abs(mean((as.vector(pred[,9]) -as.vector(y.test[,9]))/as.vector(pred[,9])))
mse10 <- abs(mean((as.vector(pred[,10]) -as.vector(y.test[,10]))/as.vector(pred[,10])))
mse11 <- abs(mean((as.vector(pred[,11]) -as.vector(y.test[,11]))/as.vector(pred[,11])))
mse12 <- abs(mean((as.vector(pred[,12]) -as.vector(y.test[,12]))/as.vector(pred[,12])))
mse13 <- abs(mean((as.vector(pred[,13]) -as.vector(y.test[,13]))/as.vector(pred[,13])))

MSE <- cbind(mse1, mse2, mse3, mse4, mse5, mse6, mse7, mse8, mse9, mse10, mse11, mse12, mse13)
colnames(MSE) <- c("x1 mape", "x2 mape", "x3 mape", "x4 mape", "x5 mape", "x6 mape", "x7 mape", "x8 mape", "x9 mape", "y1 mape", "y2 mape", "y3 mape", "Pt mape")
MSE

pred[1:20, 13]
y.test[1:20, 13]

#Reduce the mole fractions and pressure into two different models
